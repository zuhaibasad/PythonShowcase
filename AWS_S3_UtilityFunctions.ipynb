{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "AWS-S3-UtilityFunctions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVPbTY_Eb5Ma"
      },
      "source": [
        "### This Notebook contain scripts which makes data access, read, write/download from AWS easier and faster using AWS APIs. These scripts have to be modified to work for other AWS S3 buckets since folder paths/file paths, naming conventions were fixed for the former client."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "butMQMaG6cmC"
      },
      "source": [
        "!pip install boto3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Evuh3IJqOK"
      },
      "source": [
        "import os\n",
        "import boto3\n",
        "import pandas\n",
        "import datetime\n",
        "import boto3\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import json\n",
        "#import config_file as config1\n",
        "import time "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ7_yi7oUeov"
      },
      "source": [
        "## New Changes 06/15/2021 ------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBwA7WyqY3I0"
      },
      "source": [
        "timestamp_start = config1.config_pre['elsa_th']['timestamp_start']\n",
        "timestamp_end = config1.config_pre['elsa_th']['timestamp_end']\n",
        "anomaly_high = config1.config_pre['elsa_th']['anomaly_high']\n",
        "anomaly_low = config1.config_pre['elsa_th']['anomaly_low']\n",
        "result_path = config1.config_pre['elsa_th']['result_path']\n",
        "bucket_name = config1.config_pre['elsa_th']['bucket']\n",
        "anomalous_file_path = config1.config_pre['elsa_th']['anomalous_file_path']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbBFISHVInKU"
      },
      "source": [
        "s3_resource = boto3.resource('s3',\n",
        "                aws_access_key_id = aws_access_key,\n",
        "                aws_secret_access_key = aws_secret_key\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYsIg2JsqFEb"
      },
      "source": [
        "class S3_Utility:\n",
        "  def __init__(self, aws_access_key, aws_secret_key, region='us-east-2'):\n",
        "    self.aws_access_key = aws_access_key\n",
        "    self.aws_secret_key = aws_secret_key\n",
        "    self.s3_resource = boto3.resource('s3',\n",
        "                aws_access_key_id = aws_access_key,\n",
        "                aws_secret_access_key = aws_secret_key,\n",
        "                region_name = region\n",
        "            )\n",
        "    self.bucket_list = [each.name for each in self.s3_resource.buckets.all()]\n",
        " \n",
        "#  def __del__(self):\n",
        "#    self.s3_resource.\n",
        "\n",
        "  def get_buckets(self):\n",
        "    return self.s3_resource.buckets.all()\n",
        "  \n",
        "  def s3_list_files(self, bucket_name=None, Directory='', FileExtension='', timestart='1900-01-01 00:00:00', timeend='2200-01-01 00:00:00', Path=True):\n",
        "    \n",
        "    if timestart == '' or timestart is None:\n",
        "      timestart='1900-01-01 00:00:00'\n",
        "    if timeend == '' or timeend is None:\n",
        "      timeend='2200-01-01 00:00:00'\n",
        "\n",
        "    timestart = datetime.datetime.strptime(timestart+'+00:00', '%Y-%m-%d %H:%M:%S%z')\n",
        "    timeend = datetime.datetime.strptime(timeend+'+00:00', '%Y-%m-%d %H:%M:%S%z')\n",
        "\n",
        "    if (bucket_name is not None) and (bucket_name in self.bucket_list):\n",
        "      filename = []\n",
        "      if FileExtension != '':\n",
        "        if not(Path):\n",
        "          filenames = [each.key.split('/')[-1] for each in self.s3_resource.Bucket(bucket_name).objects.filter(Prefix=Directory).all() if (each.last_modified >= timestart and each.last_modified <= timeend) and each.key.split(\".\")[-1] == FileExtension]\n",
        "        else:\n",
        "          filenames = [each.key for each in self.s3_resource.Bucket(bucket_name).objects.filter(Prefix=Directory).all() if (each.last_modified >= timestart and each.last_modified <= timeend) and each.key.split(\".\")[-1] == FileExtension]\n",
        "\n",
        "      else:\n",
        "        if not(Path):\n",
        "          filenames = [each.key.split('/')[-1] for each in self.s3_resource.Bucket(bucket_name).objects.filter(Prefix=Directory).all() if (each.last_modified >= timestart and each.last_modified <= timeend) ]      \n",
        "        else:\n",
        "          filenames = [each.key for each in self.s3_resource.Bucket(bucket_name).objects.filter(Prefix=Directory).all() if (each.last_modified >= timestart and each.last_modified <= timeend) ]\n",
        "\n",
        "\n",
        "      return filenames\n",
        "    else:\n",
        "      print(\"Bucket Name is not correct!\")\n",
        "\n",
        "  \n",
        "  def s3_download_files(self, bucket_name=None, remote_path_list=[], localpath=None, isWriteSummary=1, folderStructure=True, concurrency=None):\n",
        "    # this function downloads file in a bucket.\n",
        "    # remote_path is the location of file in bucket. This must be a list\n",
        "    # localpath is path of directory in current working directory where you want to store files locallay on your system\n",
        "\n",
        "    if type(remote_path_list) == list and remote_path_list != []:\n",
        "      if (bucket_name is not None) and (bucket_name in self.bucket_list):\n",
        "      #if not os.path.exists(os.path.join(os.getcwd(), localpath)):\n",
        "        os.makedirs(localpath, exist_ok=True)\n",
        "        out = []\n",
        "        CONNECTIONS = concurrency #setting for how many concurrent downloads you want, currently it is getting 200 downloads concurrently?\n",
        "        TIMEOUT = 60\n",
        "        bucket_obj = self.s3_resource.Bucket(bucket_name)\n",
        "        #config = list(map(lambda each: (bucket_obj, each, localpath, folderStructure), remote_path_list))\n",
        "\n",
        "        def load_url(bkt, remote_file_path, local_path, fs, timeout):\n",
        "          check = [each.key for each in bkt.objects.filter(Prefix=remote_file_path).all()]\n",
        "          if check != []:\n",
        "            if fs:\n",
        "              local_path = os.path.join(local_path, check[0])\n",
        "              os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n",
        "            else:\n",
        "              local_path = os.path.join(os.getcwd(), local_path, os.path.split(remote_file_path)[-1])\n",
        "            \n",
        "            if not os.path.exists(local_path):\n",
        "              bkt.download_file(Key=remote_file_path, Filename=local_path)\n",
        "            #return [bkt.name, remote_file_path, 1]\n",
        "          else:\n",
        "            print(\"Bucket: \"+bkt.name+\" does not have this file: \"+remote_file_path+\" !\")\n",
        "            #return [bkt.name, remote_file_path, 0]\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=CONNECTIONS) as executor:\n",
        "          future_to_url = (map(lambda each: executor.submit(load_url, bucket_obj, each, localpath, folderStructure, TIMEOUT), remote_path_list ))\n",
        "          #time1 = time.time()\n",
        "          for future in concurrent.futures.as_completed(future_to_url):\n",
        "            try:\n",
        "              data = future.result()\n",
        "            except Exception as exc:\n",
        "              data = str(type(exc))\n",
        "            finally:\n",
        "              out.append(data)\n",
        "              #print(str(len(out)),end=\"\\r\")\n",
        "\n",
        "            #time2 = time.time()\n",
        "\n",
        "        #print(f'Took {time2-time1:.2f} s')\n",
        "        \n",
        "        if isWriteSummary:\n",
        "          print(out)\n",
        "          pd.DataFrame(out, columns=['BucketName', 'BucketFi33lePath', 'Status']).to_csv(os.path.join(localpath, \"Download_Summary.csv\"), index=False)\n",
        "      else:\n",
        "        print(\"Bucket Name is not correct!\")  \n",
        "    else:\n",
        "      print(\"Remote Path arguement must be a list and cannot be empty!\")\n",
        "    \n",
        "  def s3_bucket_summary(self, bucket_name=None):\n",
        "    # this function returns all files and their paths in a bucket and writes to a csv file current working directory\n",
        "    # returns None\n",
        "     \n",
        "\n",
        "    if (bucket_name is not None) and (bucket_name in self.bucket_list):\n",
        "      filenames = [[bucket_name, each.key, each.key.split(\"/\")[-1]] for each in self.s3_resource.Bucket(bucket_name).objects.filter().all()]\n",
        "      pd.DataFrame(filenames, columns=['Bukcet', 'FullPaths', 'FileNames']).to_csv(os.path.join(os.getcwd(), \"Bucket_Summary.csv\"),index=False)\n",
        "    else:\n",
        "      print(\"Bucket Name is not correct!\")\n",
        "  \n",
        "  def s3_folder_find(self, Directory='', isWriteSummary=1):\n",
        "    # this function search in all buckets for Directory and \n",
        "    # returns [bucket_name, path of that folder in bucket] also write a CSV file on \n",
        "    # current working directory\n",
        "\n",
        "    master = []\n",
        "    if Directory != '':\n",
        "      for each in self.s3_resource.buckets.all():\n",
        "        filenames = [[each.name, files.key[0:files.key.find(Directory)+len(Directory)]] for files in self.s3_resource.Bucket(each.name).objects.filter(Prefix=Directory).all()]\n",
        "        master = master + filenames\n",
        "      if isWriteSummary:\n",
        "        pd.DataFrame(master, columns=['Bucket', 'Path']).drop_duplicates().to_csv(os.path.join(os.getcwd(),'Folder_Search_results.csv'), index=False)\n",
        "      \n",
        "      return pd.DataFrame(master).drop_duplicates().values\n",
        "    else:\n",
        "      print(\"Argument Directory cannot be empty!\")\n",
        "\n",
        "  def anomaly_download(self, bucket_name=None, timestamp_start='1900-01-01 00:00:00', timestamp_end='2260-12-31 23:59:59', anomaly_low=-100000, anomaly_high=100000, result_path_folder=None, input_csv_path=None, folderStructure=True, concurrency=None):\n",
        "    ## this function downloads anomalous files filtered from csv file list \n",
        "    ## filtered parameters are timestamp range and anomalous value range\n",
        "    ## \"bucket_name\" is good to have otherwise script will search first \n",
        "    ## part of \"input_csv_file\", which is actually Floc name, in all buckets\n",
        "    ## if found in more than one bucket, then takes first bucket only to download \n",
        "    ## anomalous files. \n",
        "    ## parameter: \"input_csv_path\" is required\n",
        "    ## store downloaded files in \"result_path_folder\"\n",
        "    if timestamp_start is None or timestamp_start == '':\n",
        "      timestamp_start='1900-01-01 00:00:00'\n",
        "    \n",
        "    if timestamp_end is None or timestamp_end == '':\n",
        "      timestamp_end='2260-12-31 23:59:59'\n",
        "    \n",
        "    if anomaly_low is None or anomaly_low == '':\n",
        "      anomaly_low = -100000\n",
        "\n",
        "    if anomaly_high is None or anomaly_high == '':\n",
        "      anomaly_high = 1000000\n",
        "    \n",
        "    \n",
        "    if input_csv_path is None or input_csv_path == '' or input_csv_path.split(\".\")[-1] != 'csv':\n",
        "      print(\"Please provide anomalous file details in csv !!!\")\n",
        "      return 0\n",
        "\n",
        "    root = input_csv_path.split(\"/\")[-1].split(\"_mic\")[0]\n",
        "    if bucket_name is None or bucket_name is '':\n",
        "      buckets = self.s3_folder_find(root, 0)\n",
        "      if len(buckets) == 0:\n",
        "        print(\"Main Folder \"+root+\" is not found in any bucket !!!\")\n",
        "        return 0\n",
        "      else:\n",
        "        bucket_name = buckets[0][0]\n",
        "\n",
        "    if bucket_name not in self.bucket_list:\n",
        "      print(\"Bucket Name is not Found! Please check spellings or capatilizations in name !!!\")\n",
        "      return 0\n",
        "    \n",
        "    print(\"Bucket is :\"+bucket_name)\n",
        "    anomaly_files = pd.read_csv(input_csv_path)\n",
        "    anomaly_files['fileTimestamp'] = anomaly_files['fileTimestamp'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') )\n",
        "\n",
        "    timestamp_start = datetime.datetime.strptime(timestamp_start, '%Y-%m-%d %H:%M:%S')\n",
        "    timestamp_end = datetime.datetime.strptime(timestamp_end, '%Y-%m-%d %H:%M:%S')\n",
        "    filterFiles = anomaly_files.loc[(anomaly_files['fileTimestamp'] >= timestamp_start) & (anomaly_files['fileTimestamp'] <= timestamp_end) &\n",
        "                      (anomaly_files['fileAnomalousness'] >= anomaly_low) & (anomaly_files['fileAnomalousness'] <= anomaly_high) ]['filePath'].values\n",
        "\n",
        "\n",
        "    #filterFiles = ['trf3-recorder-2/2021-04-16/00/TRF3_mic08_2021-04-16_00.03.00.flac']\n",
        "    root = input_csv_path.split(\"/\")[-1].split(\"_mic\")[0]\n",
        "\n",
        "    filteredFiles = list(map(lambda x: root+'/'+x.replace(\"\\\\\",'/'), filterFiles ))\n",
        "    print(\"Downloading \"+str(len(filteredFiles))+\" Files !\")\n",
        "    self.s3_download_files(bucket_name=bucket_name, remote_path_list=filteredFiles, localpath=result_path_folder, isWriteSummary=0, folderStructure=folderStructure, concurrency=concurrency)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmDk7poaRTbN"
      },
      "source": [
        "test = S3_Utility(aws_access_key=aws_creds['AWS_ACCESS_KEY_ID'], aws_secret_key=aws_creds['AWS_SECRET_ACCESS_KEY'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqyIEv9_d3-K"
      },
      "source": [
        "t0= time.clock()\n",
        "\n",
        "x = test.anomaly_download(bucket_name='audiot-disk03', timestamp_start=timestamp_start, timestamp_end=timestamp_end, anomaly_high=anomaly_high, anomaly_low=anomaly_low,\n",
        "                      result_path_folder=result_path, input_csv_path='/content/TRF3_2020-12_mic09_anomalousness.csv',\n",
        "                       folderStructure=True, concurrency=1000)\n",
        "\n",
        "t1 = time.clock() - t0\n",
        "print(\"Time elapsed: \", t1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX5YdLnsoVvf"
      },
      "source": [
        "anomaly_files = pd.read_csv('TRF3_2020-12_mic09_anomalousness.csv')\n",
        "\n",
        "filterFiles = anomaly_files['filePath'].values\n",
        "    \n",
        "#filterFiles = ['trf3-recorder-2/2021-04-16/00/TRF3_mic08_2021-04-16_00.03.00.flac']\n",
        "root = 'TRF3_2020-12'\n",
        "\n",
        "filteredFiles = list(map(lambda x: root+'/'+x.replace(\"\\\\\",'/'), filterFiles ))[0:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcc-1VyoV9SU"
      },
      "source": [
        "untrained_sessions_query = \"\"\" Select bd.\\\"sessionId\\\", bd.timestamp, bd.type, bd.heart_rate, bd.qrs_interval,\n",
        "                                      bd.pr_interval, bd.cluster_group\n",
        "                               From (Select untrained.\\\"sessionId\\\" \n",
        "                               \n",
        "                                    From ( Select distinct \\\"sessionId\\\" \n",
        "                                            From %s\n",
        "                                            Where type IN (4,5,6) \n",
        "                                            Group By \\\"sessionId\\\" \n",
        "                                            Having Count(id) > %d ) untrained left join %s trained ON \n",
        "                                                                            untrained.\\\"sessionId\\\"=trained.\\\"sessionId\\\"\n",
        "                                    \n",
        "                                    Where trained.\\\"sessionId\\\" IS NULL) untr JOIN %s bd ON untr.\\\"sessionId\\\"=bd.\\\"sessionId\\\" \"\"\" % (table_beat, beat_threshold, table_mdl, table_beat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHjo14_igJZK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rLsMmU0zHPO"
      },
      "source": [
        "\n",
        "from concurrent import futures\n",
        "\n",
        "\n",
        "bucket_name = 'audiot-disk03'\n",
        "\n",
        "s3_object_keys = [] # List of S3 object keys\n",
        "\n",
        "\n",
        "def main1(s3=None, filteredFiles=None):\n",
        "  \n",
        "  result_path = 'results/elsa'\n",
        "  def fetch(key):\n",
        "      fullpath = result_path+'/'+key\n",
        "      \n",
        "      os.makedirs(os.path.split(fullpath)[0], exist_ok=True) \n",
        "      \n",
        "      with open(fullpath, 'wb') as data:\n",
        "          s3.download_fileobj(bucket_name, key, data)\n",
        "      \n",
        "      return fullpath\n",
        "\n",
        "\n",
        "  def fetch_all(keys):\n",
        "\n",
        "      with futures.ThreadPoolExecutor(max_workers=2000) as executor:\n",
        "          future_to_key = {executor.submit(fetch, key): key for key in keys}\n",
        "\n",
        "          print(\"All URLs submitted.\")\n",
        "\n",
        "          for future in futures.as_completed(future_to_key):\n",
        "\n",
        "              key = future_to_key[future]\n",
        "              exception = future.exception()\n",
        "\n",
        "              if not exception:\n",
        "                  yield key, future.result()\n",
        "              else:\n",
        "                  yield key, exception\n",
        "\n",
        "\n",
        "  for key, result in fetch_all(filteredFiles):\n",
        "      #print(f'key: {key}  result: {result}')\n",
        "      continue\n",
        "\n",
        "s3_1 = boto3.client('s3', \n",
        "                 aws_access_key_id = aws_creds['AWS_ACCESS_KEY_ID'],\n",
        "                aws_secret_access_key = aws_creds['AWS_SECRET_ACCESS_KEY'])\n",
        "s3_2 = boto3.client('s3', \n",
        "                 aws_access_key_id = aws_creds['AWS_ACCESS_KEY_ID'],\n",
        "                aws_secret_access_key = aws_creds['AWS_SECRET_ACCESS_KEY'])\n",
        "s3_3 = boto3.client('s3', \n",
        "                 aws_access_key_id = aws_creds['AWS_ACCESS_KEY_ID'],\n",
        "                aws_secret_access_key = aws_creds['AWS_SECRET_ACCESS_KEY'])\n",
        "s3_4 = boto3.client('s3', \n",
        "                 aws_access_key_id = aws_creds['AWS_ACCESS_KEY_ID'],\n",
        "                aws_secret_access_key = aws_creds['AWS_SECRET_ACCESS_KEY'])\n",
        "\n",
        "main1(s3_1, filteredFiles[0:250])\n",
        "main1(s3_2, filteredFiles[250:500])\n",
        "main1(s3_3, filteredFiles[500:750])\n",
        "main1(s3_4, filteredFiles[750:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9ned3d6Ovqb"
      },
      "source": [
        "def chunks(filteredFiles, n):\n",
        "  for i in range(0, len(filteredFiles), n):\n",
        "    yield filteredFiles[i:i + n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49x0AinyoTBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6cfe64-8475-4d6c-d109-2e218356424e"
      },
      "source": [
        "\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def api_call(id):\n",
        "    s33 = boto3.client('s3', \n",
        "                 aws_access_key_id = aws_creds['AWS_ACCESS_KEY_ID'],\n",
        "                aws_secret_access_key = aws_creds['AWS_SECRET_ACCESS_KEY'])\n",
        "    main1(s33, id)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    p = Pool(4)\n",
        "    p.map(api_call, chunks(filteredFiles, 250) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All URLs submitted.\n",
            "All URLs submitted.\n",
            "All URLs submitted.\n",
            "All URLs submitted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDmD5FoSjxHW"
      },
      "source": [
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHZORCVX6L8y"
      },
      "source": [
        "s3_resource = s3_resource = boto3.resource('s3',\n",
        "                aws_access_key_id = aws_creds['AWS_ACCESS_KEY_ID'],\n",
        "                aws_secret_access_key = aws_creds['AWS_SECRET_ACCESS_KEY']\n",
        "            )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qfaN8BNXPZQ3",
        "outputId": "4115e6be-6928-4cab-a42e-b727e00d4bb8"
      },
      "source": [
        "resp = s3_resource.Object('audiot-disk03', 'TRF3_2020-12/trf3-recorder-2/2021-01-22/00/TRF3_mic09_2021-01-22_00.00.00.flac')\n",
        "str(resp.last_modified)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2021-06-04 09:38:21+00:00'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb4ZGJ_oO69N",
        "outputId": "a5be6ee9-0d50-4e83-ec5b-0d973f910811"
      },
      "source": [
        "[each.key for each in s3_resource.Bucket('audiot-disk03').objects.filter(Prefix='TRF3_2021-02/trf3-recorder-2/2021-04-16/00/TRF3_mic08_2021-04-16_00.03.00.flac').all()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TRF3_2021-02/trf3-recorder-2/2021-04-16/00/TRF3_mic08_2021-04-16_00.03.00.flac']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAoggXi88Cyu",
        "outputId": "128b8dc6-3a9e-4f76-ee0a-06c1ed5f8ab8"
      },
      "source": [
        "[each.last_modified for each in s3_resource.Bucket('audiot-disk03').objects.filter(Prefix='TRF3_2020-12/trf3-recorder-2/2021-01-22/00/TRF3_mic09_2021-01-22_00.00.00.flac').all()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[datetime.datetime(2021, 6, 4, 9, 38, 21, tzinfo=tzlocal())]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_t5r1Vy8AOg",
        "outputId": "91d68e37-497d-4312-e2eb-36c59cbf12f8"
      },
      "source": [
        "[each.key for each in s3_resource.Bucket('audiot-disk03').objects.filter(Prefix='TRF3_2020-12/trf3-recorder-2/2021-01-22/00/TRF3_mic09_2021-01-22_00.00.00.flac').all()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TRF3_2020-12/trf3-recorder-2/2021-01-22/00/TRF3_mic09_2021-01-22_00.00.00.flac']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtLY_uzv8hcX"
      },
      "source": [
        "TRF3_2020-12\\trf3-recorder-2\\2021-01-22\\00\\TRF3_mic09_2021-01-22_00.00.00.flac"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDvq2XAwSdG2"
      },
      "source": [
        "with open('AWS_S3_creds.json', 'r') as f:\n",
        "  aws_creds = json.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI2gcAgmUjym"
      },
      "source": [
        "## End of Changes 06/15/2021 -----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g545PFbUySve"
      },
      "source": [
        "### Lists of all S3 buckets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6ckN1VsrR22",
        "outputId": "6bae4b58-61aa-4f40-ec99-6ddc5a3ac6e9"
      },
      "source": [
        "test.s3_folder_find(Directory='trf3-recorder-2\\2021-01-22\\02\\TRF3_mic09_2021-01-22_02.48.00.flac')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YeKxpsMySve",
        "outputId": "86c49b9a-e960-49de-f140-d3f50807d352"
      },
      "source": [
        "for each in test.get_buckets().all():\n",
        "  print(each.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "audiot-disk03\n",
            "audiot-practicum-2020\n",
            "audiotcostuse\n",
            "cough-data\n",
            "dshin-sagemaker\n",
            "dva-poultry-data\n",
            "guy-upload-test\n",
            "hyelimyang-sagemaker\n",
            "jasplund-sagemaker\n",
            "outside-upload-bucket\n",
            "rajeevk-newbucket1\n",
            "rkavadiki3--sagemaker\n",
            "rzhao-sagemaker\n",
            "sagemaker-studio-225277178130-8e9y2s8g15h\n",
            "sagemaker-studio-225277178130-e591ugskj6j\n",
            "sagemaker-studio-225277178130-f6v3hs0ga8n\n",
            "sagemaker-studio-225277178130-qx122t1x93n\n",
            "sagemaker-studio-us-east-1-225277178130\n",
            "sagemaker-studio-us-east-1-225277178130-ktallau-chirp\n",
            "sagemaker-studio-us-east-1-tyson-night-data\n",
            "sagemaker-studio-us-east-1-tyson-night-data-2\n",
            "sagemaker-us-east-1-225277178130\n",
            "sagemaker-us-east-2-225277178130\n",
            "sganesh48-sagemaker\n",
            "shangao-sagechicken\n",
            "tbwrf-2021-02-video\n",
            "trf0-disk2\n",
            "trf0-disk3\n",
            "tyson-night-data\n",
            "tyson-night-data-2\n",
            "vetworks-upload\n",
            "wilsonvetco-upload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyUElzrxySvg"
      },
      "source": [
        "###List of directories/sub-directories\n",
        "\n",
        "- Function Name: `s3_list_dirs`\n",
        "- Input: `bucket_name`, `sub_dir`, `paths`\n",
        "    1. `bucket_name`: e.g. 'tyson-night-data'\n",
        "    2. `sub_dir`: If True return list of directories as well as all nested subdirectories\n",
        "    3. `paths`: If True return list with complete paths\n",
        "- Output: list of directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHcgc99Hy1xB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b34f999-b8d5-433a-ca05-c4e11ac60411"
      },
      "source": [
        "def s3_list_dirs(bucket_name, sub_dir, path):\n",
        "  subdirs = []\n",
        "  dirs = []\n",
        "  paths = []\n",
        "  \n",
        "  if sub_dir:\n",
        "    bucket_info = s3_client.list_objects(Bucket=bucket_name)\n",
        "    if 'Contents' in bucket_info.keys():\n",
        "      for each in bucket_info['Contents']:\n",
        "        temp = each['Key'].split('/')\n",
        "        if path and len(temp) > 1:\n",
        "          paths.append('/'.join(each['Key'].split('/')[0:-1]))\n",
        "        \n",
        "        if len(temp) > 2:  \n",
        "          subdirs += each['Key'].split('/')[1:-1]\n",
        "    else:\n",
        "      print(\"No Sub-Directories Found in bucket: \"+bucket_name)\n",
        "      \n",
        "  bucket_info = s3_client.list_objects(Bucket=bucket_name, Delimiter='/')\n",
        "  print(bucket_info)\n",
        "  if 'CommonPrefixes' in bucket_info.keys():\n",
        "    for each in bucket_info['CommonPrefixes']:\n",
        "      dirs.append(each['Prefix'])\n",
        "  else:\n",
        "    print(\"No Directory found in bucket: \"+bucket_name)\n",
        "\n",
        "\n",
        "  if path:\n",
        "    return paths\n",
        "  \n",
        "  return list(set(dirs+subdirs))\n",
        "\n",
        "\n",
        "s3_list_dirs('audiot-disk03', False, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ResponseMetadata': {'RequestId': '2N5DR4TRZ1RP0YRH', 'HostId': '0iKSNkgkXctBoNZkbNf7lGU8JpXY3Qwd2tDhBrI7HsT9elwrgekZMI+U5PxsdPnTDknDs9Dfv8Y=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': '0iKSNkgkXctBoNZkbNf7lGU8JpXY3Qwd2tDhBrI7HsT9elwrgekZMI+U5PxsdPnTDknDs9Dfv8Y=', 'x-amz-request-id': '2N5DR4TRZ1RP0YRH', 'date': 'Mon, 14 Jun 2021 20:31:10 GMT', 'x-amz-bucket-region': 'us-east-2', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'IsTruncated': False, 'Marker': '', 'Name': 'audiot-disk03', 'Prefix': '', 'Delimiter': '/', 'MaxKeys': 1000, 'CommonPrefixes': [{'Prefix': '$RECYCLE.BIN/'}, {'Prefix': 'System Volume Information/'}, {'Prefix': 'TRF0_2020-12/'}, {'Prefix': 'TRF0_2021-02/'}, {'Prefix': 'TRF1_2020-04and07_raw_partial/'}, {'Prefix': 'TRF1_2020-12/'}, {'Prefix': 'TRF1_2021-02/'}, {'Prefix': 'TRF2_2020-12/'}, {'Prefix': 'TRF2_2021-02/'}, {'Prefix': 'TRF3_2020-12/'}, {'Prefix': 'TRF3_2021-02/'}], 'EncodingType': 'url'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "VaZePJmGySvg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhW3jLDEySvh"
      },
      "source": [
        "###  Files\n",
        "\n",
        "- Function Name: `s3_list_files`\n",
        "- Input: `bucket_name`, `file_extension`, `paths`\n",
        "    1. `bucket_name`: 'tyson-night-data'\n",
        "    2. `file_extension`: Filter files based on the given file extension, if empty then list all the files\n",
        "    3. `paths`: If True return list with complete paths otherwise just the name of the files\n",
        "- Output: list of files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwLQnOlMySvh"
      },
      "source": [
        "def s3_list_files(bucket_name, filextension, path):\n",
        "  files = []\n",
        "  paths = []\n",
        "\n",
        "  bucket_info = s3_client.list_objects(Bucket=bucket_name)\n",
        "  if 'Contents' in bucket_info.keys():\n",
        "    for each in bucket_info['Contents']:\n",
        "      temp = each['Key'].split('/')\n",
        "      if filextension == '':\n",
        "        files.append(temp[-1])\n",
        "        paths.append(each['Key'])\n",
        "\n",
        "      elif filextension != '' and filextension in temp[-1]:\n",
        "        files.append(temp[-1])\n",
        "        paths.append(each['Key'])\n",
        "  else:\n",
        "    print(\"No File Found in bucket: \"+bucket_name)\n",
        "    return []\n",
        "\n",
        "  if path:\n",
        "    return paths\n",
        "\n",
        "  return files\n",
        "\n",
        "s3_list_files('tyson-night-data', 'flac', True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b43rR8NLySvi"
      },
      "source": [
        "###  Read files\n",
        "\n",
        "- Function Name: `s3_read_files`\n",
        "- Input: `file_path`\n",
        "    1. `file_path`: \n",
        "- Output: File as vector\n",
        "\n",
        "<!-- Note:  --> We might need to discuss this once you reach this task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0XnqcDZySvi"
      },
      "source": [
        "def s3_read_files(bucket_name, file_path):\n",
        "  return [x for x in s3_client.get_object(Bucket=bucket_name , Key = file_path)['Body'].read()]\n",
        "\n",
        "s3_read_files('tyson-night-data',  'Train/2020-02-08/23/TRF0_mic11_2020-02-08_23.26.30_rzha.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VtPLG8Izo11"
      },
      "source": [
        "### Download files\n",
        "\n",
        "- Function Name: `s3_download_files`\n",
        "- Input: `remote_file_path`\n",
        "    1. `remote_file_path`: \n",
        "    2. `local file path`\n",
        "- Output: None\n",
        "\n",
        "<!-- Note:  --> We might need to discuss this once you reach this task"
      ]
    }
  ]
}