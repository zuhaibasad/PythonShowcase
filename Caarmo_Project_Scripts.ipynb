{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Caarmo_Project_Scripts.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbRfHKjIZc8X"
      },
      "source": [
        "### This Notebook is contains code which collects data from tracking devices, process the data, use Google APIs to get some demographics details, fetch data from bigquery and then stream complete, processed data into the Bigquery tables via BigQuery API.\n",
        " This code is scheduled using Google Schedulers and Pub/Sub Topics to run every 4 hours.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWu60-7Gpuad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e956dcf5-39d5-4824-eefb-0cfbeca89b1e"
      },
      "source": [
        "!pip install herepy\n",
        "!pip install mpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting herepy\n",
            "  Downloading https://files.pythonhosted.org/packages/78/c6/8d88f19a5bc508cf3ed64ea9ac5c6619e06d8aafd7a0a1db9dbda674d765/herepy-3.1.0-py3-none-any.whl\n",
            "Collecting requests==2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.24.0->herepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.24.0->herepy) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.24.0->herepy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.24.0->herepy) (3.0.4)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, herepy\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed herepy-3.1.0 requests-2.24.0\n",
            "Collecting mpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/3a/c4c04201c9cd8c5845f85915d644cb14b16200680e5fa424af01c411e140/mpu-0.23.1-py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.1MB/s \n",
            "\u001b[?25hInstalling collected packages: mpu\n",
            "Successfully installed mpu-0.23.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLfFoF4P31IC"
      },
      "source": [
        "\"\"\"Function called by PubSub trigger to execute cron job tasks.\"\"\"\n",
        "import datetime\n",
        "import herepy ## here API for python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "## mpu library for haversin_distance calulation\n",
        "from mpu import haversine_distance \n",
        "import logging\n",
        "from string import Template\n",
        "from google.cloud import bigquery\n",
        "import pandas_gbq\n",
        "from google import oauth2\n",
        "import json\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXZCKAWG5k-A"
      },
      "source": [
        "def get_streetaddress(lat, longi, address_dict, gc_reverseAPI):\n",
        "  \"\"\" this function takes latitude and longitude and converts it into\n",
        "      corressponding street address using HERE API\n",
        "      Args: lat - latitude\n",
        "          : longi - longitude\n",
        "          : address_dict: dictionary containing some address alread present\n",
        "                          in AddressDB table in big query\n",
        "          : gc_reverseAPI : object of HERE API to convert geocoordinates\n",
        "  \"\"\"\n",
        "\n",
        "  lat = str(lat)\n",
        "  longi = str(longi)\n",
        "  ## if latitude and longitude are not present\n",
        "  ## or cached already then sends requests to the HERE API\n",
        "  ## else ignore\n",
        "\n",
        "  if (lat+\",\"+longi) not in address_dict.keys():\n",
        "    try:\n",
        "      response = gc_reverseAPI.retrieve_addresses([lat, longi])\n",
        "    except Exception as e:\n",
        "      print(\"Exception !!! \", e, \" for Geocoord: \", (lat, longi))\n",
        "    else:\n",
        "      resp_dict = response.as_dict()['items'][0]['title']    \n",
        "      if resp_dict:\n",
        "        address_dict[lat+\",\"+longi] = str(resp_dict)\n",
        "      else:  \n",
        "        address_dict[lat+\",\"+longi] = \"Not Available\"\n",
        "        logging.info('Could not convert to Street Address')\n",
        "\n",
        "  return address_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lmpQjSauDce"
      },
      "source": [
        "def addressforevent_1(gc_reverseAPI, each, triptrack, address_dict):\n",
        "\t\"\"\" this function sets keys for triptrack and populate\n",
        "\t\t\tstreet_addresses in it\n",
        "\t\t\tArgs: gc_reverseAPI - HERE API object for reverse geo coordinates\n",
        "\t\t\t\t\t: each - one row of big query table\n",
        "\t\t\t\t\t: triptrack - empty dictionary\n",
        "\t\t\t\t\t: address_dict - dictionary containing Street Addresses\n",
        "\t\t\t\"\"\"\n",
        "\t\n",
        "\t## gets the street address for latitude and longitude using\n",
        "\t## function get_streetaddress and populates it into address_dict\n",
        "\taddress_dict = get_streetaddress(each['latitude'], each['longitude'], address_dict, gc_reverseAPI)\n",
        "\tstreet_address = address_dict[each['latitude']+\",\"+each['longitude']]\n",
        "\n",
        "\t## is current row deviceID is not present in triptrack\n",
        "\tif each['deviceidentification'] not in triptrack:\n",
        "\t\t## then adds a new key of deviceID and other keys\n",
        "\t\ttriptrack[each['deviceidentification']] = {each['tripnumber']: {each['eventtype']: { (each['latitude']+\",\"+each['longitude']) : {'street_address': street_address} }}}\n",
        "\telse:\n",
        "\t\t## if deviceID already present, then check if tripnumber is there\n",
        "\t\tif each['tripnumber'] not in triptrack[each['deviceidentification']]:\n",
        "\t\t\t## not there already, then adds a new key of tripnumber and other keys\n",
        "\t\t\ttriptrack[each['deviceidentification']][each['tripnumber']] = {each['eventtype']: { (each['latitude']+\",\"+each['longitude']) :{'street_address': street_address}}}\n",
        "\t\telse:\n",
        "\t\t\t## if deviceID, tripnumber were already there then if event type was already present \n",
        "\t\t\tif each['eventtype'] not in triptrack[each['deviceidentification']][each['tripnumber']].keys():\n",
        "\t\t\t\t## if eventtype is not there then adds it as new key and other keys\n",
        "\t\t\t\ttriptrack[each['deviceidentification']][each['tripnumber']][each['eventtype']] = {(each['latitude']+\",\"+each['longitude']) : {'street_address': street_address}}\n",
        "\t\t\telse:\n",
        "\t\t\t\t## if deviceID, tripnumber, eventtype were already there, then check the geocoords are aleady there\n",
        "\t\t\t\tif (each['latitude']+\",\"+each['longitude']) not in triptrack[each['deviceidentification']][each['tripnumber']][each['eventtype']]:\n",
        "\t\t\t\t\t## if not there then add them as key and value is their corresponding street address.\n",
        "\t\t\t\t\ttriptrack[each['deviceidentification']][each['tripnumber']][each['eventtype']][(each['latitude']+\",\"+each['longitude'])] = {'street_address': street_address}\n",
        "\n",
        "def addressforevent_2(gc_reverseAPI, each, triptrack, address_dict):\n",
        "\t\"\"\" this function sets keys for triptrack and populate\n",
        "\t\t\tstreet_addresses in it\n",
        "\t\t\tArgs: gc_reverseAPI - HERE API object for reverse geo coordinates\n",
        "\t\t\t: each - one row of big query table\n",
        "\t\t\t: triptrack - empty dictionary\n",
        "\t\t\t: address_dict - dictionary containing Street Addresses\n",
        "\t\"\"\"\n",
        "\t## gets the street address for latitude and longitude using\n",
        "\t## function get_streetaddress and populates it into address_dict\n",
        "\taddress_dict = get_streetaddress(each['latitude'], each['longitude'], address_dict, gc_reverseAPI)\n",
        "\tstreet_address = address_dict[each['latitude']+\",\"+each['longitude']]\n",
        "\n",
        "\t## is current row deviceID is not present in triptrack\n",
        "\tif each['deviceidentification'] not in triptrack:\n",
        "\t\t## then adds a new key of deviceID and other keys\n",
        "\t\ttriptrack[each['deviceidentification']] = {each['recordedatdate']: {each['tripnumber']: {each['eventtype']: { (each['latitude']+\",\"+each['longitude']) : {'street_address': street_address} }}}}\n",
        "\telse:\n",
        "\t\tif each['recordedatdate'] not in triptrack[each['deviceidentification']].keys():\n",
        "\t\t\ttriptrack[each['deviceidentification']][each['recordedatdate']] = {each['tripnumber']: {each['eventtype']: { (each['latitude']+\",\"+each['longitude']) : {'street_address': street_address} }}}\n",
        "\t\telse:\n",
        "\t\t\t## if deviceID already present, then check if tripnumber is there\n",
        "\t\t\tif each['tripnumber'] not in triptrack[each['deviceidentification']][each['recordedatdate']].keys():\n",
        "\t\t\t\t## not there already, then adds a new key of tripnumber and other keys\n",
        "\t\t\t\ttriptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']] = {each['eventtype']: { (each['latitude']+\",\"+each['longitude']) :{'street_address': street_address}}}\n",
        "\t\t\telse:\n",
        "\t\t\t\t## if deviceID, tripnumber were already there then if event type was already present \n",
        "\t\t\t\tif each['eventtype'] not in triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']].keys():\n",
        "\t\t\t\t\t## if eventtype is not there then adds it as new key and other keys\n",
        "\t\t\t\t\ttriptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']] = {(each['latitude']+\",\"+each['longitude']) : {'street_address': street_address}}\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t## if deviceID, tripnumber, eventtype were already there, then check the geocoords are aleady there\n",
        "\t\t\t\t\tif (each['latitude']+\",\"+each['longitude']) not in triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']].keys():\n",
        "\t\t\t\t\t\t## if not there then add them as key and value is their corresponding street address.\n",
        "\t\t\t\t\t\ttriptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']][(each['latitude']+\",\"+each['longitude'])] = {'street_address': street_address}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz-HqmTCnW2a"
      },
      "source": [
        "def distance_population(triptrack):\n",
        "\t\"\"\" this function takes a populated triptrack\n",
        "\t\t\tdictionary and inserts displacement in miles for every trip\n",
        "\t\t\tmade by deviceID\n",
        "\t\t\tArgs:\ttriptrack - a populated dictionary\n",
        "\t\"\"\"\n",
        "\n",
        "\tfor eachid in triptrack.keys():\n",
        "\t\t## for every deviceID (first level key)\n",
        "\t\tfor eachday in triptrack[eachid].keys():\n",
        "\t\t\tfor eachtrip in triptrack[eachid][eachday].keys():\n",
        "\t\t\t\t## for every tripnumber (second level key)\n",
        "\t\t\t\t## get the startevent first\n",
        "\t\t\t\tif 'TripStartEvent' in triptrack[eachid][eachday][eachtrip].keys():\n",
        "\t\t\t\t\t## get latitude and longitude of start event\n",
        "\t\t\t\t\t## which will be used as origin\n",
        "\t\t\t\t\t#print(\"Have TripStartEvent\")\n",
        "\t\t\t\t\tlat1 = list(triptrack[eachid][eachday][eachtrip]['TripStartEvent'].keys())[0].split(\",\")[0]\n",
        "\t\t\t\t\tlong1 = list(triptrack[eachid][eachday][eachtrip]['TripStartEvent'].keys())[0].split(\",\")[1]\n",
        "\t\t\t\t\t## then get the latitude and longitude of \n",
        "\t\t\t\t\t## other events of a trip which will be used as destination\n",
        "\n",
        "\t\t\t\t\tfor eachtripevent in triptrack[eachid][eachday][eachtrip].keys():\n",
        "\t\t\t\t\t\t## for every tripevent (third level key)\n",
        "\t\t\t\t\t\tfor eachgeocode in triptrack[eachid][eachday][eachtrip][eachtripevent].keys():\n",
        "\t\t\t\t\t\t\t## for every geo coordinates (fourth level key)\n",
        "\t\t\t\t\t\t\tlat2 = eachgeocode.split(\",\")[0]\n",
        "\t\t\t\t\t\t\tlong2 = eachgeocode.split(\",\")[1]\n",
        "\t\t\t\t\t\t\t## gets the displacement between two geo coordinates\n",
        "\t\t\t\t\t\t\t## displacement is straight line distance on earth calculated by\n",
        "\t\t\t\t\t\t\t## using formula named haversine_distance\n",
        "\t\t\t\t\t\t\tstraitdist = None\n",
        "\t\t\t\t\t\t\tif lat1 == '0.0' and long1 == '0.0':\n",
        "\t\t\t\t\t\t\t\tstraitdist = None\n",
        "\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\tif lat2 == '0.0' and long2 == '0.0':\n",
        "\t\t\t\t\t\t\t\t\tstraitdist = None\n",
        "\t\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\t\tstraitdist = (haversine_distance((float(lat1), float(long1)), (float(lat2), float(long2)))*0.621371)\n",
        "\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\ttriptrack[eachid][eachday][eachtrip][eachtripevent][eachgeocode].update({'distance': straitdist})\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t## if startevent is not present then there is no\n",
        "\t\t\t\t\t## mechanism to select any other event as origin\n",
        "\t\t\t\t\tfor eachtripevent in triptrack[eachid][eachday][eachtrip].keys():\n",
        "\t\t\t\t\t\tfor eachgeocode in triptrack[eachid][eachday][eachtrip][eachtripevent].keys():\n",
        "\t\t\t\t\t\t\ttriptrack[eachid][eachday][eachtrip][eachtripevent][eachgeocode].update({'distance':None})\n",
        "\n",
        "\treturn triptrack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6LjQX9PziYz"
      },
      "source": [
        "def store_newvins(query_time, bq_client):\n",
        "  \n",
        "  q_vinnumbers = '''SELECT VIN FROM `noted-casing-129313.geocode_converted_address.vindata`'''\n",
        "  new_vins = query_time['mdiobdvin'].dropna().drop_duplicates(keep='first').values\n",
        "  cached_vinnumbers = bq_client.query(q_vinnumbers).to_dataframe().values\n",
        "\t\n",
        "  ## check for any new vin number in the new data ##\n",
        "  temp = []\n",
        "  for each in new_vins:\n",
        "    if len(each) >= 17:\n",
        "      if each not in cached_vinnumbers:\n",
        "        temp.append(each)\n",
        "  #################################################\n",
        "  \n",
        "  if temp:\n",
        "    print(\"Found New VIN\")\n",
        "    logging.info(\"Found New VIN\")\n",
        "    ## reading table schema for vindata table\n",
        "    f = open('tableschema_vin.json', 'r')\n",
        "    tableschema_vin = json.loads(f.read())\n",
        "    f.close()\n",
        "    #################################################\n",
        "    \n",
        "    ## getting new vindata from API and storing them \n",
        "    ## in vindata table\n",
        "    master_vindata = []\n",
        "    for each in new_vins:\n",
        "      url = 'https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVINValues/'+each+'?format=json'\n",
        "      r = requests.get(url);\n",
        "      vindata = json.loads(r.text)\n",
        "      ## the data is at key Results and \n",
        "      ## contains only 1 element in a list\n",
        "      vindata = vindata['Results'][0]\n",
        "      column_header = list(vindata.keys()) \n",
        "      column_value = list(vindata.values())\n",
        "      for i in range(0,len(column_value)):\n",
        "        if column_value[i] == '':\n",
        "          column_value[i] = None\n",
        "\n",
        "      master_vindata.append(column_value)\n",
        "    \n",
        "    df_vin = pd.DataFrame(master_vindata)\n",
        "    df_vin.columns = column_header\n",
        "    pandas_gbq.to_gbq(df_vin, destination_table='geocode_converted_address.vindata',\n",
        "                        project_id='noted-casing-129313', if_exists='append', table_schema=tableschema_vin,\n",
        "                        credentials=oauth2.service_account.Credentials.from_service_account_file('noted-casing-129313-71580930e027.json'))\t\n",
        "    print(\"Successfully stored New VINs in the table vindata\")\n",
        "    logging.info(\"Successfully stored New VINs in the table vindata\")\n",
        "  else:\n",
        "    print(\"No new VIN found ...\")\n",
        "    logging.info(\"No new VIN found ...\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyhonNhJ8Y6J"
      },
      "source": [
        "def getANDstore_address_dict(address_dict, use_flag, bq_client):\n",
        "  ## Query string to get those address which are already \n",
        "  ## present in AddressDB table and their latitude, longitude coordinates\n",
        "  ## are in new rows.\n",
        "  q_address = '''Select Distinct x.Latitude, x.Longitude, x.Street_Address \n",
        "\t\t\t\t\t\t\t\t\tFrom `noted-casing-129313.geocode_converted_address.AddressDB` x INNER JOIN\n",
        "                  \t(SELECT latitude, longitude FROM `noted-casing-129313.stitch_caarmo.obd_data_track`\n",
        "\t\t\t\t            \tWhere recordedat >= (Select MAX(recordedat) \n",
        "\t\t\t\t \t\t\t\t                           FROM `noted-casing-129313.geocode_converted_address.test_obd`)\n",
        "\t\t\t\t            \tAND longitude IS NOT NULL AND LATITUDE IS NOT NULL AND LENGTH(cast(deviceidentification as string)) = 10) y \n",
        "                  ON x.latitude = y.latitude AND x.longitude = y.longitude'''\n",
        "\n",
        "\n",
        "  query_addr = bq_client.query(q_address).to_dataframe()\n",
        "  \n",
        "  if use_flag == 0:\n",
        "    logging.info('%d addresses were cached already!', len(query_addr))\n",
        "    print(len(query_addr), 'addresses were cached already!!')\n",
        "    ## making a dictionary named address_dict of Street address\n",
        "    ## where keys is (latitude, longitude) and \n",
        "    ## value is the corresponding street address\n",
        "    ## of those geo coordinates and thi also\n",
        "    ## avoid repeated values\n",
        "\n",
        "    address_dict = {}\n",
        "    for each in query_addr.values:\n",
        "      lat = each[0]\n",
        "      longi = each[1]\n",
        "      address = each[2]\n",
        "      address_dict[str(lat+\",\"+longi)] = address\n",
        "\n",
        "    return address_dict\n",
        "  elif use_flag == 1:\n",
        "    ## deletes some addresses from address_dict\n",
        "    ## which are already present in the AddressDB\n",
        "    ## so that we will store only new address and no\n",
        "    ## duplicates to save space\n",
        "\n",
        "    for each in query_addr.values:\n",
        "      lat = each[0]\n",
        "      longi = each[1]\n",
        "      address = each[2]\n",
        "      if str(lat+\",\"+longi) in address_dict.keys():\n",
        "        del address_dict[str(lat+\",\"+longi)]\n",
        "\t\n",
        "    ## makes a dataframe from address_dict to send\n",
        "    ## it to the bigquery\n",
        "    master_address = []\n",
        "    for key in address_dict.keys():\n",
        "      lat = key.split(\",\")[0]\n",
        "      longi = key.split(\",\")[1]\n",
        "      master_address.append([lat, longi, address_dict[key]])\n",
        "    \n",
        "    df_addr = pd.DataFrame(master_address, columns=['Latitude', 'Longitude', 'Street_Address'])\n",
        "    logging.info(\"%d many address new to write!\", len(df_addr))\n",
        "    print(len(df_addr), \"many address new to write!\")\n",
        "    ## stores new discovered addresses to AddressDB\n",
        "    pandas_gbq.to_gbq(df_addr, destination_table='geocode_converted_address.AddressDB',\n",
        "                      project_id='noted-casing-129313', if_exists='append', \n",
        "                      credentials=oauth2.service_account.Credentials.from_service_account_file('noted-casing-129313-71580930e027.json'))\t\n",
        "    \n",
        "    logging.info(\"AddressDB Updated Successfully\")\n",
        "    print(\"AddressDB Updated Successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZXSg5GhXTYV"
      },
      "source": [
        "def join_obd_vindata(bq_client):\n",
        "  ## Update VIN rows and data\n",
        "\tquery_obd_vin = '''SELECT a.*, b.AirBagLocCurtain, b.AirBagLocFront,\tb.AirBagLocKnee,\tb.AirBagLocSide,\n",
        "\t\t\t\t\t\tb.ABS,\tb.AxleConfiguration,\tb.Axles,\tb.BasePrice,\t\n",
        "\t\t\t\t\t\tb.BatteryA,\tb.BedLengthIN,\tb.BodyCabType, b.BodyClass,\t\n",
        "\t\t\t\t\t\tb.BrakeSystemDesc,b.BrakeSystemType,\tb.CurbWeightLB,\tb.DaytimeRunningLight,\n",
        "\t\t\t\t\t\tb.DisplacementCC, b.DisplacementCI, b.DisplacementL,\tb.Doors,\n",
        "\t\t\t\t\t\tb.DriveType, b.EngineConfiguration,\tb.EngineCylinders,\tb.EngineHP,\t\n",
        "\t\t\t\t\t\tb.EngineManufacturer,\tb.EngineModel,\tb.ErrorText,b.FuelInjectionType,\n",
        "\t\t\t\t\t\tb.FuelTypePrimary, b.GVWR,\tb.LaneDepartureWarning,\tb.LaneKeepSystem,\t\n",
        "\t\t\t\t\t\tb.LowerBeamHeadlampLightSource, b.Make,\tb.Manufacturer,\tb.ManufacturerId,\n",
        "\t\t\t\t\t\tb.Model,\tb.ModelYear,\tb.ParkAssist, b.PedestrianAutomaticEmergencyBraking,\n",
        "\t\t\t\t\t\tb.PlantCompanyName,\tb.PlantCountry,\tb.PlantState,\tb.Pretensioner,\t\n",
        "\t\t\t\t\t\tb.SeatBeltsAll, b.Series,\tb.TPMS,\tb.TractionControl,\n",
        "\t\t\t\t\t\tb.TrailerBodyType,\tb.TrailerLength,\tb.TrailerType, b.TransmissionSpeeds,\n",
        "\t\t\t\t\t\tb.TransmissionStyle,\tb.Trim2,\tb.VIN,\tb.VehicleType,\t\n",
        "\t\t\t\t\t\tb.WheelBaseLong,\tb.WheelBaseShort,\tb.WheelBaseType, b.WheelSizeFront,\n",
        "\t\t\t\t\t\tb.WheelSizeRear,\tb.Wheels,\tb.Windows\n",
        "\t\t FROM `noted-casing-129313.geocode_converted_address.test_obd` a \n",
        "\t\t \t\t\tLEFT OUTER JOIN `noted-casing-129313.geocode_converted_address.vindata` b\n",
        "\t\t\t\t\tON a.mdiobdvin = b.VIN '''\n",
        "\tjob_config = bigquery.QueryJobConfig(destination='noted-casing-129313.geocode_converted_address.obd_data_vin', write_disposition='WRITE_TRUNCATE')\n",
        "\tquery_job = bq_client.query(query_obd_vin, job_config=job_config)\n",
        "\tquery_job.result()\n",
        "\tprint(\"obd_vin_data table has been successfully updated\")\n",
        "\tlogging.info(\"obd_vin_data table has been successfully updated\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh7Cvo6uuJ-F"
      },
      "source": [
        "def execute_query(bq_client, gc_reverseAPI, table_schema):\n",
        "\t\"\"\"this functions executes different queries and update tables\n",
        "\t\t accordingly.\n",
        "\tArgs:\n",
        "\t    bq_client: Object representing a reference to a BigQuery Client\n",
        "\t\t\tgc_reverseAPI : Object representing a reference to a HERE reverse Geocode API\n",
        "\t\t\ttable_scheme : table schema of our big query table named temporary_obdtrack\n",
        "\t\"\"\"\n",
        "\t\n",
        "\t## Query string to get current table size\n",
        "\tq_len = '''Select Count(*) From `noted-casing-129313.geocode_converted_address.test_obd` '''\n",
        "\t## makes a request through big query client and converts results to dataframe\n",
        "\t## gets the values stored in dataframe in integer\n",
        "\t## which is length of current table\n",
        "\toldlen = bq_client.query(q_len).to_dataframe().values[0]\n",
        "\n",
        "\n",
        "\t## Query string to get new rows in the table only for danlaw devices\n",
        "\tq_time = ''' SELECT * FROM `noted-casing-129313.stitch_caarmo.obd_data_track` \n",
        "\t\t\t\t \t\t\t\tWhere recordedat < (Select MAX(recordedat) \n",
        "\t\t\t\t \t\t\t\t\t FROM `noted-casing-129313.geocode_converted_address.test_obd`)\n",
        "\t\t\t\t AND longitude IS NOT NULL AND LATITUDE IS NOT NULL AND LENGTH(cast(deviceidentification as string)) = 10\n",
        "\t\t\t\t '''\n",
        "\n",
        "\tquery_time = bq_client.query(q_time).to_dataframe()\n",
        "\tnewrows = len(query_time)\n",
        "\n",
        "\t## Date conversion to YYYY-MM-DD from recordedat column\n",
        "\tquery_time['recordedatdate'] = query_time.apply(lambda each: pd.to_datetime(each['recordedat'],format='%Y-%m-%d').strftime(format='%Y-%m-%d'), axis=1)\t\n",
        "\tquery_time['recordedatdatetime'] = query_time.apply(lambda each: pd.to_datetime(each['recordedat']).strftime(format='%Y-%m-%d %H:%M:%S'), axis=1)\n",
        "\t## logging.info inserts the given string into the big query logs\n",
        "\t#print(667 in query_time['tripnumber'].values)\n",
        "\tlogging.info('%d new rows has discovered!', newrows)\n",
        "\tprint(len(query_time), ' new rows has discovered!')\n",
        " \n",
        "\t## Checks for any new VIN in the new data and\n",
        "\t## stored them in the table\n",
        "\tstore_newvins(query_time, bq_client)\n",
        "\n",
        "\t## see function get_address_dict for more details\n",
        "\t## address_dict is a dictionary containing all cached\n",
        "\t## street address of geo coordinates\n",
        "\taddress_dict = {}\n",
        "\taddress_dict = getANDstore_address_dict(address_dict, 0, bq_client)\n",
        "\t\t\n",
        "\t## A dictionary triptrack which has keys of the form\n",
        "\t## deviceid, tripnumber, eventtype, (latitude,longitude)\n",
        "\t## which makes easy to calculate displacement of each trip\n",
        "\t## for every deviceID\n",
        "\ttriptrack = {}\n",
        "\n",
        "\t## query_time is a dataframe, and apply is a method which applies \n",
        "\t## function to every row in dataframe. Here we are only populating  \n",
        "\t## dictionaty triptrack using addressforevent function.\n",
        "\tprint(\"checking addresssforevent_2\")\n",
        "\tquery_time.apply(lambda x : addressforevent_2(gc_reverseAPI ,x, triptrack, address_dict) , axis=1)\n",
        "\tprint(\"passed addressforevent_2\")\n",
        "\t## now we have populated triptrack, where all keys and street address\n",
        "\t## are stored in triptrack, now adding displacement radius\n",
        "\t## in it using distance_population function\n",
        "\ttriptrack = distance_population(triptrack)\n",
        "\n",
        "\t## new column in query_time dataframe named 'Street_Address'\n",
        "\t## in which all street addresses of latitude and longitude\n",
        "\t## are stored\n",
        "\tquery_time['Street_Address'] = query_time.apply(lambda each : triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']][each['latitude']+\",\"+each['longitude'] ]['street_address'], axis=1)\n",
        "\t\n",
        "\t## Displacement of all trips for every deviceID\n",
        "\tquery_time['Displacement'] = query_time.apply(lambda each : triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']][each['latitude']+\",\"+each['longitude'] ]['distance'], axis=1)\n",
        "\t\n",
        "\t## Store new street address into AddressDB\n",
        "\t## see function for more details \n",
        "\tgetANDstore_address_dict(address_dict,1, bq_client)\n",
        "\n",
        "\t## this function takes dataframe query_time and append it to the table\n",
        "\t## test_obd using our google cloud credientials and table_schema\n",
        "\tprint(len(query_time.columns))\n",
        "\tprint(len(table_schema))\n",
        "\tpandas_gbq.to_gbq(query_time, destination_table='geocode_converted_address.test_obd2',\n",
        "\t                      project_id='noted-casing-129313', if_exists='append', table_schema = table_schema, \n",
        "\t                      credentials=oauth2.service_account.Credentials.from_service_account_file('noted-casing-129313-71580930e027.json'))\n",
        "\tlogging.info(\"test_obd table has been updated successfully\")\n",
        "\tprint(\"test_obd table has been updated successfully\")\n",
        "\t\n",
        "\t## Query to get new length of the table\n",
        "\tq_len = '''Select Count(*) From `noted-casing-129313.geocode_converted_address.test_obd` '''\n",
        "\tnewlen = bq_client.query(q_len).to_dataframe().values[0]\n",
        "\t\n",
        "\t## A check to check if all rows have been successfully\n",
        "\t## inserted into the table, therefore, old table length and\n",
        "\t## newrows sum must be equal to newlength of the table\n",
        "\tif (newrows + oldlen) == newlen:\n",
        "\t\t## if true then delete the old temporary_obdtrack\n",
        "\t\tbq_client.delete_table('noted-casing-129313.geocode_converted_address.temporary_obdtrack', not_found_ok=True)\n",
        "\t\tprint('Table deleted successfully')\n",
        "\t\tlogging.info('Table deleted successfully')\n",
        "\n",
        "\t\t## And copy the updated test_obd table to temporary_obdtrack\n",
        "\t\tjob = bq_client.copy_table('noted-casing-129313.geocode_converted_address.test_obd', \n",
        "                        'noted-casing-129313.geocode_converted_address.temporary_obdtrack')\n",
        "\t\tjob.result()  # Wait for the job to complete.\n",
        "\t\tprint('Table copied successfully')\n",
        "\t\tlogging.info('Table copied successfully')\n",
        "\n",
        "\t\t## Now update the obd_vin_date table\n",
        "\t\tjoin_obd_vindata(bq_client)\n",
        "\n",
        "\telse:\n",
        "\t\t## if lengths are not equal, then log an error\n",
        "\t\t## in bigquery logs.\n",
        "\t\tprint('Table not copied!')\n",
        "\t\tlogging.error('Table not updated because length mismatch!')\t\n",
        "\t\n",
        "\tprint(\"Function Completed!\")\n",
        "\tlogging.info('Function Completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdrPi-xwuLhr"
      },
      "source": [
        "def main(data, context):\n",
        "\t\"\"\" Caller Function which calls and execute queries from\n",
        "\t\t\tother functions\"\"\"\n",
        "\t\n",
        "\t## API key obtained from HERE Maps account\n",
        "\t\n",
        "\t#HERE_API_Key = 'yaOI3bLuFytv7ad3TsDkG8Of5jAGm-v1yvtoinisSzw'\n",
        "\tHERE_API_Key = '3FBo3taRxPUecY98ccDjcLEhM8I4x1TLGf0lQ3103Lg'\n",
        "\t## object of bigquery API regiestered with our google cloud\n",
        "\t## account credientials in file 'noted-casing-129313-71580930e027.json'\n",
        "\t## through this client we will make reuquest of query to big query table\n",
        "\tbq_client = bigquery.Client.from_service_account_json('noted-casing-129313-71580930e027.json')\n",
        "\t\n",
        "\t## this object is for HERE API requests\n",
        "\tgc_reverseAPI = herepy.GeocoderReverseApi(HERE_API_Key)\n",
        "\n",
        "#\ttry:\n",
        "\tcurrent_time = datetime.datetime.utcnow()\n",
        "\tlog_message = Template('Cloud Function was triggered on $time')\n",
        "\tlogging.info(log_message.safe_substitute(time=current_time))\n",
        "\n",
        "#\t\ttry:\n",
        "\t\t\t## table schema of our big query table\n",
        "\t\t\t## stored in json, this is used to tell\n",
        "\t\t\t## by a function (appending data to table) to \n",
        "\t\t\t## correctly append every column\n",
        "\n",
        "\tf = open('tableschema.json')\n",
        "\ttable_schema = json.loads(f.read())\n",
        "\tf.close()\n",
        "\n",
        "\texecute_query(bq_client, gc_reverseAPI, table_schema)\n",
        "\n",
        "#\t\texcept Exception as error:\n",
        "#\t\t\tlog_message = Template('Query failed due to '\n",
        "#\t\t\t\t\t\t\t\t\t'$message.')\n",
        "#\t\t\tlogging.error(log_message.safe_substitute(message=error))\n",
        "\n",
        "#\texcept Exception as error:\n",
        "#\t\tlog_message = Template('$error').substitute(error=error)\n",
        "#\t\tlogging.error(log_message)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain('data','context')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OupYIi1GiH_J"
      },
      "source": [
        "bq_client = bigquery.Client.from_service_account_json('noted-casing-129313-71580930e027.json')\n",
        "\n",
        "q_time = ''' Select * FROM `noted-casing-129313.geocode_converted_address.test_obd` '''\n",
        "\n",
        "query_time = bq_client.query(q_time).to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_8GuJU-icfq"
      },
      "source": [
        "def poptriptrack2(each, triptrack):\n",
        "  ## is current row deviceID is not present in triptrack\n",
        "  if each['deviceidentification'] not in triptrack:\n",
        "    ## then adds a new key of deviceID and other keys\n",
        "    triptrack[each['deviceidentification']] = {each['recordedatdate']: {each['tripnumber']: {each['eventtype']: { (each['latitude']+\",\"+each['longitude']) : {'street_address': ''} }}}}\n",
        "  else:\n",
        "    if each['recordedatdate'] not in triptrack[each['deviceidentification']].keys():\n",
        "      triptrack[each['deviceidentification']][each['recordedatdate']] = {each['tripnumber']: {each['eventtype']: { (each['latitude']+\",\"+each['longitude']) : {'street_address': ''} }}}\n",
        "    else:\n",
        "      ## if deviceID already present, then check if tripnumber is there\n",
        "      if each['tripnumber'] not in triptrack[each['deviceidentification']][each['recordedatdate']].keys():\n",
        "        ## not there already, then adds a new key of tripnumber and other keys\n",
        "        triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']] = {each['eventtype']: { (each['latitude']+\",\"+each['longitude']) :{'street_address': ''}}}\n",
        "      else:\n",
        "        ## if deviceID, tripnumber were already there then if event type was already present \n",
        "        if each['eventtype'] not in triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']].keys():\n",
        "          ## if eventtype is not there then adds it as new key and other keys\n",
        "          triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']] = {(each['latitude']+\",\"+each['longitude']) : {'street_address': ''}}\n",
        "        else:\n",
        "          ## if deviceID, tripnumber, eventtype were already there, then check the geocoords are aleady there\n",
        "          if (each['latitude']+\",\"+each['longitude']) not in triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']].keys():\n",
        "            ## if not there then add them as key and value is their corresponding street address.\n",
        "            triptrack[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']][(each['latitude']+\",\"+each['longitude'])] = {'street_address': ''}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnd2dINKlEff"
      },
      "source": [
        "triptrack2 = {}\n",
        "query_time.apply(lambda each: poptriptrack2(each, triptrack2), axis=1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvigAKUmuH50"
      },
      "source": [
        "def distance_population(triptrack):\n",
        "\t\"\"\" this function takes a populated triptrack\n",
        "\t\t\tdictionary and inserts displacement in miles for every trip\n",
        "\t\t\tmade by deviceID\n",
        "\t\t\tArgs:\ttriptrack - a populated dictionary\n",
        "\t\"\"\"\n",
        "\n",
        "\tfor eachid in triptrack.keys():\n",
        "\t\t## for every deviceID (first level key)\n",
        "\t\tfor eachday in triptrack[eachid].keys():\n",
        "\t\t\tfor eachtrip in triptrack[eachid][eachday].keys():\n",
        "\t\t\t\t## for every tripnumber (second level key)\n",
        "\t\t\t\t## get the startevent first\n",
        "\t\t\t\tif 'TripStartEvent' in triptrack[eachid][eachday][eachtrip].keys():\n",
        "\t\t\t\t\t## get latitude and longitude of start event\n",
        "\t\t\t\t\t## which will be used as origin\n",
        "\t\t\t\t\tprint(\"Have TripStartEvent\")\n",
        "\t\t\t\t\tlat1 = list(triptrack[eachid][eachday][eachtrip]['TripStartEvent'].keys())[0].split(\",\")[0]\n",
        "\t\t\t\t\tlong1 = list(triptrack[eachid][eachday][eachtrip]['TripStartEvent'].keys())[0].split(\",\")[1]\n",
        "\t\t\t\t\t## then get the latitude and longitude of \n",
        "\t\t\t\t\t## other events of a trip which will be used as destination\n",
        "\n",
        "\t\t\t\t\tfor eachtripevent in triptrack[eachid][eachday][eachtrip].keys():\n",
        "\t\t\t\t\t\t## for every tripevent (third level key)\n",
        "\t\t\t\t\t\tfor eachgeocode in triptrack[eachid][eachday][eachtrip][eachtripevent].keys():\n",
        "\t\t\t\t\t\t\t## for every geo coordinates (fourth level key)\n",
        "\t\t\t\t\t\t\tlat2 = eachgeocode.split(\",\")[0]\n",
        "\t\t\t\t\t\t\tlong2 = eachgeocode.split(\",\")[1]\n",
        "\t\t\t\t\t\t\t## gets the displacement between two geo coordinates\n",
        "\t\t\t\t\t\t\t## displacement is straight line distance on earth calculated by\n",
        "\t\t\t\t\t\t\t## using formula named haversine_distance\n",
        "\t\t\t\t\t\t\tstraitdist = None\n",
        "\t\t\t\t\t\t\tif lat1 == '0.0' and long1 == '0.0':\n",
        "\t\t\t\t\t\t\t\tstraitdist = None\n",
        "\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\tif lat2 == '0.0' and long2 == '0.0':\n",
        "\t\t\t\t\t\t\t\t\tstraitdist = None\n",
        "\t\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\t\tstraitdist = (haversine_distance((float(lat1), float(long1)), (float(lat2), float(long2)))*0.621371)\n",
        "\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\ttriptrack[eachid][eachday][eachtrip][eachtripevent][eachgeocode].update({'distance': straitdist})\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t## if startevent is not present then there is no\n",
        "\t\t\t\t\t## mechanism to select any other event as origin\n",
        "\t\t\t\t\tfor eachtripevent in triptrack[eachid][eachday][eachtrip].keys():\n",
        "\t\t\t\t\t\tfor eachgeocode in triptrack[eachid][eachday][eachtrip][eachtripevent].keys():\n",
        "\t\t\t\t\t\t\ttriptrack[eachid][eachday][eachtrip][eachtripevent][eachgeocode].update({'distance':None})\n",
        "\n",
        "\treturn triptrack\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUPOqD0bmE93"
      },
      "source": [
        "query_time['Displacement'] = query_time.apply(lambda each : triptrack2[each['deviceidentification']][each['recordedatdate']][each['tripnumber']][each['eventtype']][each['latitude']+\",\"+each['longitude'] ]['distance'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDOngJBMmxaz"
      },
      "source": [
        "f = open('tableschema.json')\n",
        "table_schema = json.loads(f.read())\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNmEuBJjmZP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86385def-9aa8-4724-eb4e-4171ef7bff2d"
      },
      "source": [
        "pandas_gbq.to_gbq(query_time, destination_table='geocode_converted_address.test_obd2',\n",
        "\t                      project_id='noted-casing-129313', if_exists='replace', table_schema = table_schema, \n",
        "\t                      credentials=oauth2.service_account.Credentials.from_service_account_file('noted-casing-129313-71580930e027.json'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1it [00:44, 44.91s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNUuaVkO9O2t"
      },
      "source": [
        "#VIN, Device ID (danlaw only), Trip ID, mileage, Year, make, model, \n",
        "#MIL (all fields in OBD_Track with \"MIL\"), DTC (all fields with DTC), time, date, LAT, LONG,\n",
        "\n",
        "q_time = ''' Select * FROM `noted-casing-129313.geocode_converted_address.test_obd` a Left Outer JOIN \n",
        "              `noted-casing-129313.geocode_converted_address.vindata` b ON\n",
        "              a.mdiobdvin=b.VIN\n",
        "              '''\n",
        "\n",
        "query_time = bq_client.query(q_time).to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEYEaPljNLMT"
      },
      "source": [
        "colselect = ['VIN', 'deviceidentification', 'tripnumber', 'Displacement', 'ModelYear', 'Make', 'Model',\n",
        "              'recordedat','recordedatdate', 'latitude', 'longitude', 'eventtype']+[x for x in query_time.columns if ('dtc' in x or 'mil' in x)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMSgmuCWFK77"
      },
      "source": [
        "query_time = query_time[colselect]\\\n",
        "              .loc[ (query_time['recordedatdate']>='2020-05-23') & (query_time['recordedatdate']<='2020-05-29')].drop_duplicates().sort_values(['recordedatdate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv4S8JFfH5Uu"
      },
      "source": [
        "query_time['timestamp'] = query_time['recordedat'].apply(lambda x: x.split('T')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcSTHZ6aP26z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "3dcf53f0-373d-4c28-81e2-d1141e471135"
      },
      "source": [
        "query_time[['timestamp', 'recordedat']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>recordedat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>602</th>\n",
              "      <td>00:02:47Z</td>\n",
              "      <td>2020-05-23T00:02:47Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2300</th>\n",
              "      <td>19:06:55Z</td>\n",
              "      <td>2020-05-23T19:06:55Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2299</th>\n",
              "      <td>19:06:45Z</td>\n",
              "      <td>2020-05-23T19:06:45Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2298</th>\n",
              "      <td>19:06:26Z</td>\n",
              "      <td>2020-05-23T19:06:26Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2297</th>\n",
              "      <td>19:05:26Z</td>\n",
              "      <td>2020-05-23T19:05:26Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10540</th>\n",
              "      <td>17:23:06Z</td>\n",
              "      <td>2020-05-29T17:23:06Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10541</th>\n",
              "      <td>17:23:16Z</td>\n",
              "      <td>2020-05-29T17:23:16Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10542</th>\n",
              "      <td>17:23:32Z</td>\n",
              "      <td>2020-05-29T17:23:32Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14641</th>\n",
              "      <td>11:58:30Z</td>\n",
              "      <td>2020-05-29T11:58:30Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14576</th>\n",
              "      <td>10:58:29Z</td>\n",
              "      <td>2020-05-29T10:58:29Z</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12807 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       timestamp            recordedat\n",
              "602    00:02:47Z  2020-05-23T00:02:47Z\n",
              "2300   19:06:55Z  2020-05-23T19:06:55Z\n",
              "2299   19:06:45Z  2020-05-23T19:06:45Z\n",
              "2298   19:06:26Z  2020-05-23T19:06:26Z\n",
              "2297   19:05:26Z  2020-05-23T19:05:26Z\n",
              "...          ...                   ...\n",
              "10540  17:23:06Z  2020-05-29T17:23:06Z\n",
              "10541  17:23:16Z  2020-05-29T17:23:16Z\n",
              "10542  17:23:32Z  2020-05-29T17:23:32Z\n",
              "14641  11:58:30Z  2020-05-29T11:58:30Z\n",
              "14576  10:58:29Z  2020-05-29T10:58:29Z\n",
              "\n",
              "[12807 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szbfhBspRxD2"
      },
      "source": [
        "query_time.drop('recordedat',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpWFNvHiTV4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42bbdaa5-ddb1-4e3c-9f1e-ce8738d5708e"
      },
      "source": [
        "query_time.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12807, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcQBw2K8R53K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09117efc-232c-44f5-fd94-06f6d1a581bb"
      },
      "source": [
        "query_time.drop_duplicates().shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12807, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnGh8PtpQpLE"
      },
      "source": [
        "query_time.to_excel('VinData.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}